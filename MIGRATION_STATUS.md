# Migration Status

## Completed Steps

### âœ… Step 1: Directory Structure Created

All directories and `__init__.py` files have been created:

```
src/spatter/
â”œâ”€â”€ extraction/         # Video â†’ HDF5 pipeline
â”œâ”€â”€ data/              # Dataset & data loading
â”œâ”€â”€ models/            # Neural network architectures
â”œâ”€â”€ training/          # Training infrastructure
â”œâ”€â”€ inference/         # Production inference
â”œâ”€â”€ hpo/               # Hyperparameter optimization
â”œâ”€â”€ semisupervised/    # Label refinement
â””â”€â”€ utils/             # Shared utilities

src/api/
â”œâ”€â”€ routes/            # API endpoints
â””â”€â”€ ...

config/
â”œâ”€â”€ model/             # Model configurations
â”œâ”€â”€ training/          # Training configurations
â”œâ”€â”€ data/              # Data configurations
â””â”€â”€ experiment/        # Experiment configurations

scripts/               # CLI entry points
tests/
â”œâ”€â”€ unit/              # Unit tests
â””â”€â”€ integration/       # Integration tests
data/
â”œâ”€â”€ raw/videos/        # Original videos
â”œâ”€â”€ processed/hdf5/    # Extracted spatters (598,933 images, 2.1GB)
â”œâ”€â”€ models/checkpoints/# Saved models
â””â”€â”€ predictions/       # Inference outputs
legacy/                # Archived research code
docs/                  # Documentation
docker/                # Docker configs
.github/workflows/     # CI/CD
```

### âœ… Step 2: Legacy Code Archived

- **Moved** `1 spatter extraction/` â†’ `legacy/1 spatter extraction/`
- **Moved** `2 cnn/` â†’ `legacy/2 cnn/`
- **Created** `legacy/README.md` with usage instructions

### âœ… Step 3: Data Migrated

- **Copied** 6 HDF5 files (2.1GB total) â†’ `data/processed/hdf5/`
  - `left_low_0.h5` (101,430 images)
  - `left_high_2.h5` (106,594 images)
  - `mid_low_0.h5` (101,534 images)
  - `mid_high_2.h5` (91,784 images)
  - `right_low_0.h5` (104,003 images)
  - `right_high_2.h5` (93,588 images)
- **Copied** CSV index files:
  - `images.csv` (shuffled - for training)
  - `unshuffled_images.csv` (debugging)

### âœ… Step 4: Documentation Organized

- **Moved** to `docs/`:
  - `ANALYSIS.md` - Technical deep-dive
  - `ARCHITECTURE.md` - Project structure
  - `AGENT_CONTEXT.json` - Machine-readable specs
  - `analysis_report.json` - Metrics
  - `DOCUMENTATION_INDEX.md` - Navigation
  - `analysis_sample_images.png` - Sample visualizations

### âœ… Step 5: Package Configuration

Updated `pyproject.toml` with:
- **Package structure**: `src/spatter` as main package
- **Optional dependencies**: Groups for training, mlops, hpo, api, dev
- **CLI entry points**: `spatter-extract`, `spatter-train`, `spatter-predict`
- **Testing config**: pytest, coverage, mypy, ruff
- **Build system**: hatchling

### âœ… Step 6: .gitignore Updated

Configured to ignore:
- Python artifacts (`__pycache__`, `.pyc`)
- Virtual environments
- Large data files (`.h5`, `.mp4`, `.avi`)
- Model checkpoints (`.pth`, `.onnx`)
- MLflow/Hydra outputs
- IDE files

---

## Remaining Tasks

### ðŸ”² Step 7: Migrate Core Code

#### 7.1 Extraction Pipeline
**Source**: `legacy/2 cnn/spatter_fvideo_hdf5_bytes_v3.py` (346 lines)
**Target**: Refactor into modular components:
- `src/spatter/extraction/pipeline.py` - Main orchestrator
- `src/spatter/extraction/frame_processor.py` - Frame differencing
- `src/spatter/extraction/filters.py` - Size/color filtering
- `src/spatter/extraction/storage.py` - HDF5 writer
- `src/spatter/extraction/video_reader.py` - Video I/O

**Key changes**:
- Extract hardcoded parameters â†’ config files
- Add logging instead of print statements
- Add type hints
- Add error handling
- Make testable (dependency injection)

#### 7.2 Dataset Implementation
**Source**: `legacy/2 cnn/spatter_dataset.py` (427 lines)
**Target**:
- `src/spatter/data/dataset.py` - Core Dataset classes
- `src/spatter/data/transforms.py` - Transform pipeline
- `src/spatter/data/datamodule.py` - PyTorch Lightning DataModule

**Key changes**:
- Keep lazy loading pattern (critical for performance)
- Externalize transform config
- Add data validation
- Add type hints
- Support CPU/MPS/CUDA (per user request in CLAUDE.md:390)

#### 7.3 Model Architecture
**Source**: `legacy/2 cnn/lenet5.py` (63 lines)
**Target**:
- `src/spatter/models/lenet5.py` - Winner model
- `src/spatter/models/base.py` - Abstract base class
- `src/spatter/models/registry.py` - Model factory

**Key changes**:
- Device-agnostic (CPU/MPS/CUDA support)
- Add model registry for easy experimentation
- Add ONNX export method
- Add type hints

#### 7.4 Training Script
**Source**: `legacy/2 cnn/train_lenet5_ModelTraining.ipynb`
**Target**:
- `scripts/train.py` - Main CLI entry point
- `src/spatter/training/trainer.py` - Training loop
- `src/spatter/training/metrics.py` - Evaluation metrics
- `src/spatter/training/callbacks.py` - Checkpointing, early stopping

**Key changes**:
- Convert notebook â†’ reproducible script
- Add MLflow/WandB experiment tracking
- Add Hydra configuration
- Support CPU/MPS/CUDA
- Add progress bars (tqdm)
- Add model checkpointing

### ðŸ”² Step 8: Configuration Files

Create Hydra configs:
- `config/config.yaml` - Base configuration
- `config/data/extraction.yaml` - Extraction parameters
- `config/data/dataset.yaml` - Dataset parameters
- `config/model/lenet5.yaml` - Model architecture
- `config/training/default.yaml` - Training hyperparameters
- `config/experiment/baseline.yaml` - Baseline experiment

### ðŸ”² Step 9: Entry Point Scripts

Create CLI scripts:
- `scripts/extract_spatters.py` - Video â†’ HDF5 extraction
- `scripts/train.py` - Model training
- `scripts/evaluate.py` - Model evaluation
- `scripts/predict.py` - Inference on new data
- `scripts/export_onnx.py` - Model export

### ðŸ”² Step 10: Testing Infrastructure

Create test suite:
- `tests/unit/test_dataset.py` - Dataset tests
- `tests/unit/test_transforms.py` - Transform tests
- `tests/unit/test_models.py` - Model tests
- `tests/integration/test_training_pipeline.py` - End-to-end training
- `tests/conftest.py` - Pytest fixtures

### ðŸ”² Step 11: Package Installation

Install the package in editable mode:
```bash
# Install core dependencies
uv sync

# Install training dependencies
uv pip install -e ".[training]"

# Install all dependencies
uv pip install -e ".[all]"
```

---

## Current Project State

### File Counts
- **Legacy code**: 2 directories (archived)
- **Data files**: 6 HDF5 files (2.1GB) + 2 CSV files
- **Documentation**: 6 files in `docs/`
- **Package structure**: Created, not yet populated

### Next Immediate Action

**Start with Step 7.2: Migrate Dataset** - This is the foundation for training.

Rationale:
1. Dataset is already well-structured (lazy loading works)
2. Minimal changes needed (mostly adding type hints, config)
3. Required for both training and inference
4. Easy to test in isolation

Then proceed to:
- Step 7.3: Migrate models (depends on dataset)
- Step 8: Create base config files (needed for training)
- Step 7.4: Create training script (combines dataset + model + config)

---

## Installation Commands

After migration is complete:

```bash
# 1. Install package in editable mode with training dependencies
uv pip install -e ".[training]"

# 2. Verify installation
python -c "from spatter.data import dataset; print('Dataset imported successfully')"

# 3. Run extraction (when implemented)
spatter-extract video.mp4 --config config/data/extraction.yaml

# 4. Run training (when implemented)
spatter-train --config config/experiment/baseline.yaml

# 5. Run inference (when implemented)
spatter-predict video.mp4 --model data/models/checkpoints/best_model.pth
```

---

## Benefits Achieved So Far

âœ… **Clean structure**: Professional package layout
âœ… **Dependency management**: Optional dependency groups
âœ… **Legacy preserved**: Original code safe in `legacy/`
âœ… **Data organized**: Clear separation of raw vs processed
âœ… **Documentation centralized**: All docs in one place
âœ… **CLI ready**: Entry points defined in pyproject.toml
âœ… **Testing ready**: Test directories and config in place
âœ… **CI/CD ready**: `.github/workflows/` directory created

---

**See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for full details.**
